{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moussala data parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we will use something from https://github.com/ODZ-UJF-AV-CR/AIRDOS_calibration/blob/master/airdos_parser/AIRDOS4_256_flux.ipynb\n",
    "\n",
    "Steps to get Liulin data:\n",
    "- get Liulin data from Todor\n",
    "- concatenate them and convert the timestamps in the result (see transform-date.sh)\n",
    "\n",
    "Steps to get Airdos data:\n",
    "- use Martin's downloader script\n",
    "\n",
    "OR: Steps to get Airdos data:\n",
    "- retrieve the data from space.astro.cz using the get_data.sh (note that these will be GB of data)\n",
    "- concatenate the data via \"find space.astro.cz -name '*A2_meta.csv' -exec mv '{}' A2/ \\;\" and cat as needed.\n",
    "\n",
    "Note:\n",
    "Integration time for one record should be 10.24 s (pers. comm. with MK on 180702).\n",
    "\n",
    "Then this notebook makes a HDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables\n",
    "import datetime\n",
    "from IPython.display import display, Math, Latex\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse AIRDOS data and save them to HDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flux( fstore, hdfkey, fna, NOISE_LEVEL ):\n",
    "    # Chunked flux computing from a CSV file\n",
    "    print('Computing flux for '+fna)\n",
    "    fna_chunksize = 100000\n",
    "    dfa_chunks = pd.read_csv(fna, sep=',', header=None, comment='*', parse_dates=[0], error_bad_lines=False,chunksize=fna_chunksize)\n",
    "\n",
    "    DATA_LEVEL=256\n",
    "    LAST_CHANNEL=514\n",
    "    surface = 2.0 # cm2\n",
    "    dtime=10.24 # s\n",
    "\n",
    "    store = pd.HDFStore(fstore,mode='a')\n",
    "    \n",
    "    dfa = pd.DataFrame()\n",
    "    lines_parsed = 0\n",
    "    \n",
    "    for chunk in dfa_chunks:\n",
    "        dfa_chunk = pd.DataFrame()\n",
    "        # Compute particle flux, assuming the data are inbetween NOISE_LEVEL and LAST_LEVEL channels\n",
    "        dfa_chunk = chunk.iloc[:,DATA_LEVEL:LAST_CHANNEL]\n",
    "        dfa_chunk['flux'] = chunk.iloc[:,NOISE_LEVEL:LAST_CHANNEL].sum(axis=1).astype('float64')\n",
    "        dfa_chunk['flux'] = dfa_chunk['flux']/(surface*dtime)\n",
    "        dfa_chunk['timestamp'] = pd.to_datetime(chunk.iloc[:,0], unit='s', errors='coerce')\n",
    "        # Drop all events with any NaN/NaT events\n",
    "        dfa_chunk.dropna(inplace=True)\n",
    "        dfa_chunk.set_index('timestamp',drop=False,inplace=True)\n",
    "        #dfa = pd.concat([dfa,dfa_chunk])\n",
    "        store.append(hdfkey,dfa_chunk,format='table',append=True,complevel=9, complib='blosc')\n",
    "        lines_parsed += len(dfa_chunk.index)\n",
    "        del dfa_chunk\n",
    "        sys.stdout.write('.')\n",
    "        gc.collect()\n",
    "\n",
    "    print(' ')\n",
    "    print(repr(lines_parsed)+' records parsed.')\n",
    "    print('')\n",
    "    del dfa_chunks\n",
    "    store.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data for Liulin\n",
      "Computing flux for A1-2017-2018-04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 12375: expected 521 fields, saw 916\n",
      "\n",
      "Skipping line 14838: expected 521 fields, saw 1040\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......... \n",
      "871117 records parsed.\n",
      "\n",
      "Computing flux for A2-2017-2018-04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 12360: expected 521 fields, saw 917\n",
      "\n",
      "Skipping line 17177: expected 521 fields, saw 562\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 781443: expected 521 fields, saw 884\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. \n",
      "870366 records parsed.\n",
      "\n",
      "All data saved to 'moussala.h5'\n"
     ]
    }
   ],
   "source": [
    "# Data file to store the data\n",
    "hdf = 'moussala.h5'\n",
    "pd.set_option('mode.chained_assignment','raise')\n",
    "\n",
    "print('Parsing data for Liulin')\n",
    "store = pd.HDFStore(hdf,mode='w')\n",
    "df = pd.read_csv('liulin-all.dat', sep=',', header=0, parse_dates=[0])\n",
    "df.set_index('timestamp',drop=False,inplace=True)\n",
    "store.put('liulin', df, format='t',complevel=9, complib='blosc')\n",
    "store.close()\n",
    "del(df)\n",
    "gc.collect()\n",
    "\n",
    "compute_flux(hdf,'A1','A1-2017-2018-04.csv', 267)\n",
    "compute_flux(hdf,'A2','A2-2017-2018-04.csv', 267)\n",
    "\n",
    "print('All data saved to '+repr(hdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
